{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['text', 'random', 'flatten', 'choose', 'stem', 'entropy', 'Text', 'draw', 'find', 'load']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.tag import *\n",
    "from nltk import *\n",
    "from nltk.tokenize import *\n",
    "%pylab inline\n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.column_space', 50)\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "set_matplotlib_formats('retina')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# gensim modules# gensi \n",
    "import gensim\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('train.tsv')\n",
    "test = pd.read_table('test.tsv')\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "test.columns = test.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wanted_tags = ['NNP', 'NNPS'] #NNP stands for Proper Nouns\n",
    "\n",
    "\n",
    "## Create NNP based on pos tags. NNP = proper nouns\n",
    "df['NNP'] = df.phrase.apply(lambda x: ' '.join(['NNP' if pos in wanted_tags else word for word, pos in pos_tag(word_tokenize(x))]))\n",
    "'-----------------------------------------'\n",
    "test['NNP'] = test.phrase.apply(lambda x: ' '.join(['NNP' if pos in wanted_tags else word for word, pos in pos_tag(word_tokenize(x))]))\n",
    "\n",
    "\n",
    "##filter out duplicate NNP\n",
    "df['NNP'] = df.NNP.str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1')\n",
    "'-----------------------------------------'\n",
    "test['NNP'] = test.NNP.str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1')\n",
    "\n",
    "\n",
    "## Turn NNPs into NNP\n",
    "df.NNP = df.NNP.str.replace('NNPs','NNP')\n",
    "'-------------------------------------------------------------'\n",
    "test.NNP = test.NNP.str.replace('NNPs','NNP')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## replace certain words and get rid of unnecessary words or spaces \n",
    "df.NNP = df.NNP.str.replace(\" '\", \"\") \n",
    "df.NNP = df.NNP.str.replace(\" n't\", \"n't\")\n",
    "df.NNP = df.NNP.str.replace(\"-LRB-\", \"\")\n",
    "df.NNP = df.NNP.str.replace(\"-RRB-\", \"\")\n",
    "df.NNP = df.NNP.str.replace(\".\", \"\")\n",
    "'-----------------------------------------'\n",
    "test.NNP = test.NNP.str.replace(\" '\", \"\") \n",
    "test.NNP = test.NNP.str.replace(\" n't\", \"n't\")\n",
    "test.NNP = test.NNP.str.replace(\"-LRB-\", \"\")\n",
    "test.NNP = test.NNP.str.replace(\"-RRB-\", \"\")\n",
    "test.NNP = test.NNP.str.replace(\".\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "# turn `` into \"\" because there are movie names like `` Analyze That '' which in order for regex to find Analyze That (name of movie), you need to turn `` into \"\"\n",
    "df.NNP = df.NNP.str.replace(\"``\", '\"')\n",
    "'-----------------------------------------'\n",
    "test.NNP = test.NNP.str.replace(\"``\", '\"')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Turn ' Analyze That (movie name) ' into NNP\n",
    "df['NNP'] = df.NNP.str.replace(r'\"(.*?)\"', 'NNP')\n",
    "'-----------------------------------------'\n",
    "test['NNP'] = test.NNP.str.replace(r'\"(.*?)\"', 'NNP')\n",
    "\n",
    "\n",
    "# Get rid of duplicate NNPs again\n",
    "df['NNP'] = df.NNP.str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1')\n",
    "'-----------------------------------------'\n",
    "test['NNP'] = test.NNP.str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos Tag didn't take filter all of the proper nouns. The code below is me manually getting all the named entity through this following condition: if first word is capitalized and the next word is capitalized, and so is the next on and so on, all these words become Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "namedent = set()\n",
    "\n",
    "for i in df.index:\n",
    "    namedent_words = []\n",
    "    sentence = df.NNP[i].split()\n",
    "\n",
    "    for index, word in enumerate(sentence):\n",
    "        next_index = index + 1\n",
    "        if word.istitle():\n",
    "            if index < len(sentence) -1:\n",
    "                if sentence[next_index].istitle():\n",
    "                    if sentence[index] not in namedent_words:\n",
    "                        namedent_words.append(sentence[index])\n",
    "                    if sentence[next_index] not in namedent_words:\n",
    "                        namedent_words.append(sentence[next_index])\n",
    "                        \n",
    "    if namedent_words:\n",
    "        namedent_joined = ' '.join(namedent_words)\n",
    "        namedent.add(namedent_joined)\n",
    "\n",
    "'------------------------------------------------------------------------------------'\n",
    "\n",
    "test_namedent = set()\n",
    "\n",
    "for i in test.index:\n",
    "    test_namedent_words = []\n",
    "    sentence = test.NNP[i].split()\n",
    "\n",
    "    for index, word in enumerate(sentence):\n",
    "        next_index = index + 1\n",
    "        if word.istitle():\n",
    "            if index < len(sentence) -1:\n",
    "                if sentence[next_index].istitle():\n",
    "                    if sentence[index] not in test_namedent_words:\n",
    "                        test_namedent_words.append(sentence[index])\n",
    "                    if sentence[next_index] not in test_namedent_words:\n",
    "                        test_namedent_words.append(sentence[next_index])\n",
    "                        \n",
    "    if test_namedent_words:\n",
    "        test_namedent_joined = ' '.join(test_namedent_words)\n",
    "        test_namedent.add(test_namedent_joined)\n",
    "        \n",
    "        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change leftover Proper nouns into NNP\n",
    "df.NNP = df.NNP.apply(lambda x: 'NNP' if x in namedent else x)\n",
    "'-------------------------------------------------------------'\n",
    "test.NNP = test.NNP.apply(lambda x: 'NNP' if x in test_namedent else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of punctuations\n",
    "\n",
    "df.NNP = df.NNP.apply(lambda x: re.sub(r'\\s([?.!,''\"](?:\\s|$))', r'\\1', x))\n",
    "'----------------------------------------------------------------------------------'\n",
    "test.NNP = test.NNP.apply(lambda x: re.sub(r'\\s([?.!,''\"](?:\\s|$))', r'\\1', x))\n",
    "\n",
    "\n",
    "df.NNP = df.NNP.str.replace(\",\", \"\")\n",
    "'-----------------------------------------'\n",
    "test.NNP = test.NNP.str.replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for tokenized NNP-ed words for faster processing (such as for Stanford's NER taging)\n",
    "\n",
    "df['NNP_tokens'] = df.NNP.apply(lambda x: word_tokenize(x.lower()))\n",
    "df['NNP'] = df.NNP.apply(lambda x: x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NNP.to_csv('rt_review.txt', header=None, index=None, sep=' ', mode='a')\n",
    "test.NNP.to_csv('test_rt_review.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_train.csv')\n",
    "test.to_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec / Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LabeledLineSentence (object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'rt_review.txt':'review', 'test_rt_review.txt':'test_review'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a049de3c5849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \"\"\"\n\u001b[1;32m    800\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 801\u001b[0;31m             sentences, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1485\u001b[0m                 )\n\u001b[1;32m   1486\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('doc2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_model = gensim.models.Doc2Vec.load('doc2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('good', 0.7179684042930603),\n",
       " ('neat', 0.6981481313705444),\n",
       " ('cool', 0.6815207004547119),\n",
       " ('great', 0.6599352359771729),\n",
       " ('interesting', 0.5582364201545715),\n",
       " ('little', 0.5564978718757629),\n",
       " ('fine', 0.546832263469696),\n",
       " ('decent', 0.5333112478256226),\n",
       " ('also', 0.5299546718597412),\n",
       " ('plus', 0.5282900929450989)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model.most_similar('nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((25000, 100))\n",
    "train_labels = numpy.zeros(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_model['TRAIN_NEG_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12500):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    train_arrays[i] = model[prefix_train_pos]\n",
    "    train_arrays[12500 + i] = model[prefix_train_neg]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arrays = numpy.zeros((25000, 100))\n",
    "test_labels = numpy.zeros(25000)\n",
    "\n",
    "for i in range(12500):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    test_arrays[i] = model[prefix_test_pos]\n",
    "    test_arrays[12500 + i] = model[prefix_test_neg]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[12500 + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84696"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim with RT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = {'rt_review.txt':'reviews'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LabeledLineSentence at 0x10f0586d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:535: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=model.corpus_count, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rt_doc2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load('rt_doc2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('intentions', 0.5827995538711548),\n",
       " ('very', 0.5787571668624878),\n",
       " ('\"good', 0.5731457471847534),\n",
       " ('\"does', 0.560712456703186),\n",
       " ('bad', 0.5394617319107056),\n",
       " ('great', 0.537720799446106),\n",
       " ('for', 0.5320193767547607),\n",
       " ('ye', 0.5192350745201111),\n",
       " ('but', 0.5140767097473145),\n",
       " ('still', 0.5119826793670654)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_polarity.csv')\n",
    "\n",
    "df.NNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = np.zeros((df.NNP.shape[0], 100))\n",
    "train_labels = np.array(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df.NNP.shape[0]):\n",
    "    train_arrays[i] = model['reviews_' + str(i)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_arrays, train_labels, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver = 'newton-cg', multi_class= 'multinomial')\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5363321799307958"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48811354607202356"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "# from src.word_embedding_utils_v2 import clean_corpus, build_w2id_dict, tokenize_text_data\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set_ids = np.loadtxt('word2vec_rtreview.txt', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dev_set_ids).to_pickle('rtreview_word2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_model = Doc2Vec.load('rt_doc2vecmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = w2vec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pad_sequences(dev_set_ids, maxlen= 100, padding='post', truncating='post', value=0)\n",
    "df = pd.read_csv('df_polarity.csv')\n",
    "labels = pd.get_dummies(df.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dev_set_ids, labels, \n",
    "                                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wv_matrix(genism_model):\n",
    "    #build the np matrix\n",
    "    embedding_shape = (len(genism_model.wv.vocab), genism_model.trainables.layer1_size)\n",
    "    embedding_matrix = np.zeros(embedding_shape)\n",
    "\n",
    "    #insert the data from model:\n",
    "    for index in range(len(genism_model.wv.vocab)):\n",
    "        embedding_vector = genism_model.wv[genism_model.wv.index2word[index]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    return(embedding_matrix)\n",
    "\n",
    "\n",
    "embedding_matrix = get_wv_matrix(w2vec_model)\n",
    "\n",
    "embeddings = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1],\n",
    "                    weights=[embedding_matrix], input_length = 100)\n",
    "\n",
    "\n",
    "\n",
    "#great now lets run the CNN\n",
    "cnn_test = Sequential()\n",
    "\n",
    "#first conv layer + max pool\n",
    "cnn_test.add(embeddings)\n",
    "cnn_test.add(Conv1D(filters=100, kernel_size = 5, activation = 'relu', strides = 1))\n",
    "cnn_test.add(MaxPooling1D(5))\n",
    "\n",
    "#2nd conv layer + max pooling\n",
    "cnn_test.add(Conv1D(filters =100, kernel_size = 2, activation='relu'))\n",
    "cnn_test.add(MaxPooling1D(5))\n",
    "\n",
    "#flatten and then connect\n",
    "cnn_test.add(Flatten())\n",
    "cnn_test.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "#output layer with sigmoid activation\n",
    "cnn_test.add(Dense(y_train.shape[1], activation = 'sigmoid'))\n",
    "\n",
    "# Compile settings\n",
    "print('\\tcompiler settings complete!')\n",
    "cnn_test.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cnn_test.fit(x_train, y_train, validation_data=(x_validate, y_validate), epochs = 5, batch_size= 1000, verbose=2)\n",
    "cnn_test.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haebichan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31212/31212 [==============================] - 9s 283us/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test, batch_size = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(pred, axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_test = pd.read_csv('df_test.csv')\n",
    "\n",
    "# df_train = df_train[df_train.NNP.notnull()]\n",
    "# df_test = df_test[df_test.NNP.notnull()]\n",
    "\n",
    "df_train = df_train.fillna('.')\n",
    "df_test = df_test.fillna('.')\n",
    "\n",
    "train_text = df_train.NNP.values\n",
    "test_text = df_test.NNP.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=df_train.sentiment.values\n",
    "y=to_categorical(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848,) (124848, 5)\n",
      "(31212,) (31212, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_text,X_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2,stratify=y)\n",
    "print(X_train_text.shape,y_train.shape)\n",
    "print(X_val_text.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15950"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words=' '.join(X_train_text)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "num_unique_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_len=[]\n",
    "for text in X_train_text:\n",
    "\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "MAX_REVIEW_LEN=np.max(r_len)\n",
    "MAX_REVIEW_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = num_unique_word\n",
    "max_words = MAX_REVIEW_LEN\n",
    "batch_size = 128\n",
    "epochs = 3\n",
    "num_classes=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train_text))\n",
    "X_train = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_val = tokenizer.texts_to_sequences(X_val_text)\n",
    "X_test = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124843, 48) (31211, 48) (66286, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1593700   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 64)          42240     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 1,648,521\n",
      "Trainable params: 1,648,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1=Sequential()\n",
    "model1.add(Embedding(max_features,100,mask_zero=True))\n",
    "model1.add(LSTM(64,dropout=0.4, recurrent_dropout=0.4,return_sequences=True))\n",
    "model1.add(LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model1.add(Dense(num_classes,activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124843 samples, validate on 31211 samples\n",
      "Epoch 1/1\n",
      "124843/124843 [==============================] - 210s 2ms/step - loss: 1.0475 - acc: 0.5845 - val_loss: 0.8825 - val_acc: 0.6448\n",
      "CPU times: user 8min 18s, sys: 59.7 s, total: 9min 18s\n",
      "Wall time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history1=model1.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=1, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66286/66286 [==============================] - 73s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred =model1.predict_classes(X_test,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66286,)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 2)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# submission = pd.read_csv('sampleSubmission.csv')\n",
    "\n",
    "# submission.Sentiment=y_pred\n",
    "# submission.to_csv('submission.csv',index=False)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
